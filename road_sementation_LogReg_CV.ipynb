{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import os,sys\n",
    "# from PIL import Image\n",
    "\n",
    "from helpers import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\prisgdd\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from plots import cross_validation_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import cv2\n",
    "except: \n",
    "    import pip\n",
    "    pip.main(['install', 'opencv-python'])\n",
    "    import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 satellite + ground truth images\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"training/\"\n",
    "image_dir = root_dir + \"images/\"\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "\n",
    "files = os.listdir(image_dir)\n",
    "\n",
    "n = len(files)\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "\n",
    "print(\"Loading \" + str(n) + \" satellite + ground truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = 16\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 99999.99999999999\n",
      "\n",
      "1-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\prisgdd\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing done!\n",
      "\n",
      "2-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n",
      "Postprocessing done!\n",
      "\n",
      "3-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n",
      "Postprocessing done!\n",
      "\n",
      "4-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n",
      "Postprocessing done!\n",
      "\n",
      "5-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n",
      "Postprocessing done!\n",
      "\n",
      "6-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n",
      "Postprocessing done!\n",
      "\n",
      "7-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n",
      "Postprocessing done!\n",
      "\n",
      "8-th CV\n",
      "Got all data in arrays!\n",
      "Train data ready!\n",
      "Test data ready!\n",
      "Model fitted!\n",
      "Postprocessing done!\n",
      "\n",
      "9-th CV\n",
      "Got all data in arrays!\n"
     ]
    }
   ],
   "source": [
    "# Cs = np.arange(1e4, 1e4 + 1, 10000) \n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "Cs = [1/lambda_ for lambda_ in lambdas]\n",
    "\n",
    "acc_threshold =[]\n",
    "\n",
    "accuracy_train_C = []\n",
    "f1_score_train_C = []\n",
    "\n",
    "accuracy_test_C = []\n",
    "f1_score_test_C = []\n",
    "\n",
    "for C in Cs:\n",
    "    print(\"C = {}\".format(C))\n",
    "\n",
    "    accuracy_train_CV = []\n",
    "    accuracy_test_CV = []\n",
    "\n",
    "    f1_score_train_CV = []\n",
    "    f1_score_test_CV = []\n",
    "    \n",
    "    accuracy_train_CV_pp = []\n",
    "    accuracy_test_CV_pp = []\n",
    "    \n",
    "    \n",
    "    f1_score_train_CV_pp = []\n",
    "    f1_score_test_CV_pp = []\n",
    "\n",
    "    for ind, [train_index, test_index] in enumerate(kf.split(imgs)):\n",
    "\n",
    "        #\n",
    "        # Split dataset for Cross Validation\n",
    "        #\n",
    "        print(\"\\n{}-th CV\".format(ind+1))\n",
    "\n",
    "        X_train = [imgs[ind] for ind in train_index]\n",
    "        X_test = [imgs[ind] for ind in test_index]\n",
    "\n",
    "        y_train = [gt_imgs[ind] for ind in train_index]\n",
    "        y_test = [gt_imgs[ind] for ind in test_index]\n",
    "\n",
    "        #\n",
    "        # Crop images, extract features, features augmentation and standardization\n",
    "        # For both train & test datasets\n",
    "        # \n",
    "        X_train = [img_crop(X_train[i], patch_size, patch_size, step = 4) for i in range(len(train_index))]\n",
    "        y_train = [img_crop(y_train[i], patch_size, patch_size, step = 4) for i in range(len(train_index))]\n",
    "        X_test = [img_crop(X_test[i], patch_size, patch_size) for i in range(len(test_index))]\n",
    "        y_test = [img_crop(y_test[i], patch_size, patch_size) for i in range(len(test_index))]\n",
    "\n",
    "        X_train = np.asarray([X_train[i][j] for i in range(len(X_train)) for j in range(len(X_train[i]))])\n",
    "        X_test = np.asarray([X_test[i][j] for i in range(len(X_test)) for j in range(len(X_test[i]))])\n",
    "        y_train = np.asarray([y_train[i][j] for i in range(len(y_train)) for j in range(len(y_train[i]))])\n",
    "        y_test = np.asarray([y_test[i][j] for i in range(len(y_test)) for j in range(len(y_test[i]))])\n",
    "\n",
    "        y_train = np.asarray([value_to_class(np.mean(y_train[i])) for i in range(y_train.shape[0])])\n",
    "        y_test = np.asarray([value_to_class(np.mean(y_test[i])) for i in range(y_test.shape[0])])\n",
    "        print(\"Got all data in arrays!\")\n",
    "        \n",
    "        X_train = np.asarray([ extract_features(X_train[i]) for i in range(len(X_train))])\n",
    "        X_train = features_augmentation(X_train)\n",
    "        X_train -= np.mean(X_train)\n",
    "        X_train /= np.std(X_train, axis = 0)\n",
    "        print(\"Train data ready!\")\n",
    "\n",
    "        X_test = np.asarray([ extract_features(X_test[i]) for i in range(len(X_test))])\n",
    "        X_test = features_augmentation(X_test)\n",
    "        X_test -= np.mean(X_test)\n",
    "        X_test /= np.std(X_test, axis = 0)\n",
    "        print(\"Test data ready!\")\n",
    "\n",
    "        # \n",
    "        # Run logistic regression \n",
    "        # \n",
    "        logreg = linear_model.LogisticRegression(C=C, class_weight=\"balanced\")\n",
    "        logreg.fit(X_train, y_train)\n",
    "        print(\"Model fitted!\")\n",
    "        z_train = logreg.predict(X_train)\n",
    "        z_test = logreg.predict(X_test)\n",
    "\n",
    "        # \n",
    "        # Compute f1 score & accuracy using sklearn functions\n",
    "        # \n",
    "        f1_score_train = f1_score(y_train, z_train, average='macro')\n",
    "        accuracy_score_train = accuracy_score(y_train, z_train)\n",
    "        f1_score_test = f1_score(y_test, z_test, average='macro')\n",
    "        accuracy_score_test = accuracy_score(y_test, z_test)\n",
    "        \n",
    "        # \n",
    "        # Post processing on test dataset\n",
    "        # \n",
    "        \n",
    "        # Reshape prediction as matrix for each image\n",
    "        z_reshaped = []\n",
    "        \n",
    "        num_patch_total = len(z_test)\n",
    "        num_patch_by_img = num_patch_total // kf.get_n_splits(imgs)\n",
    "        \n",
    "        for i in range(0, num_patch_total, num_patch_by_img):\n",
    "            z_crt = z_test[i : i + num_patch_by_img]\n",
    "            z_reshaped.append(np.reshape(z_crt, [400 // 16, 400 // 16]))\n",
    "\n",
    "        # Run post process \n",
    "        for ind, label_img in enumerate(z_reshaped):\n",
    "            label_img = postprocess(label_img)\n",
    "            z_reshaped[ind] = np.reshape(label_img, [z_crt.shape[0]])\n",
    "\n",
    "        # Convert list as array\n",
    "        z_test_pp = np.concatenate( z_reshaped , axis = 0 )\n",
    "        print(\"Postprocessing done!\")\n",
    "\n",
    "        f1_score_test_pp = f1_score(y_test, z_test_pp, average='macro')\n",
    "        accuracy_score_test_pp = accuracy_score(y_test, z_test_pp)\n",
    "        \n",
    "        \n",
    "        # \n",
    "        # Store accuracy for train, test and test+PP\n",
    "        # \n",
    "        f1_score_train_CV.append(f1_score_train)\n",
    "        accuracy_train_CV.append(accuracy_score_train)\n",
    "\n",
    "        f1_score_test_CV.append(f1_score_test)\n",
    "        accuracy_test_CV.append(accuracy_score_test)\n",
    "        \n",
    "        f1_score_test_CV_pp.append(f1_score_test_pp)\n",
    "        accuracy_test_CV_pp.append(accuracy_score_test_pp)\n",
    "    \n",
    "    print(\"Average test accuracy: {}\".format(np.mean(accuracy_test_CV)))\n",
    "    print(\"Variance test accuracy: {}\".format(np.std(accuracy_test_CV)))\n",
    "    print(\"Min test accuracy: {} // Max test accuracy: {}\\n\".format(np.min(accuracy_test_CV), np.max(accuracy_test_CV)))\n",
    "    \n",
    "    print(\"Average test accuracy PP: {}\".format(np.mean(accuracy_test_CV_pp)))\n",
    "    print(\"Variance test accuracy PP: {}\".format(np.std(accuracy_test_CV_pp)))\n",
    "    print(\"Min test accuracy PP: {} // Max test accuracy PP: {}\\n\".format(np.min(accuracy_test_CV), np.max(accuracy_test_CV)))\n",
    "    \n",
    "    accuracy_train_C.append(np.mean(accuracy_train_CV))\n",
    "    f1_score_train_C.append(np.mean(f1_score_train_CV))\n",
    "    \n",
    "    accuracy_test_C.append(np.mean(accuracy_test_CV))\n",
    "    f1_score_test_C.append(np.mean(f1_score_test_CV))\n",
    "        \n",
    "        \n",
    "# cross_validation_visualization(Cs, f1_score_train_C, f1_score_test_C)\n",
    "#     # we create an instance of the classifier and fit the data\n",
    "#     logreg = linear_model.LogisticRegression(C=1e5, class_weight=\"balanced\")\n",
    "#     logreg.fit(X, Y)\n",
    "    \n",
    "#     # Predict on the training set\n",
    "#     Z = logreg.predict(X)\n",
    "    \n",
    "#     acc = accuracy(labels = Y, predictions = Z)\n",
    "#     acc_threshold.append(acc)\n",
    "#     print(\"Foreground threshold = {}\".format(threshold))\n",
    "#     print(\"Accuracy post Logistic Regression: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_visualization(Cs, accuracy_train_C, accuracy_test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data to evaluate\n",
    "root_testdir = \"test_set_images\"\n",
    "test_names = os.listdir(root_testdir)\n",
    "num_test = len(test_names)\n",
    "\n",
    "#\n",
    "# Reorder test data \n",
    "# (Loaded in alphabetic order, but we want them in numeric order)\n",
    "# \n",
    "print(\"Loading test data\")\n",
    "order = [int(test_names[i].split(\"_\")[1]) for i in range(num_test)]\n",
    "p = np.argsort(order)\n",
    "imgs_test = [load_image(os.path.join(root_testdir, test_names[i], test_names[i]) + \".png\") for i in range(num_test)]\n",
    "\n",
    "imgs_test = [imgs_test[i] for i in p]\n",
    "img_patches_test = [img_crop(imgs_test[i], patch_size, patch_size) for i in range(num_test)]\n",
    "\n",
    "# \n",
    "# Linearize list of patches\n",
    "img_patches_test = np.asarray([img_patches_test[i][j] for i in range(len(img_patches_test)) for j in range(len(img_patches_test[i]))])\n",
    "\n",
    "print(\"Extraction, augmentation and standardization of features\")\n",
    "X_test = np.asarray([ extract_features(img_patches_test[i]) for i in range(len(img_patches_test))])\n",
    "X_test = features_augmentation(X_test)\n",
    "X_test -= np.mean(X_test)\n",
    "X_test /= np.std(X_test, axis = 0)\n",
    "\n",
    "\n",
    "print(\"Let's predict our new data\")\n",
    "# Run prediction\n",
    "Z_test = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Postprocessing \"\"\"\n",
    "\n",
    "# Reshape prediction\n",
    "Z_reshaped = []\n",
    "\n",
    "num_patch_total = len(Z_test)\n",
    "num_patch_by_img = num_patch_total // num_test\n",
    "\n",
    "for i in range(0, num_patch_total, num_patch_by_img):\n",
    "    Z_crt = Z_test[i : i + num_patch_by_img]\n",
    "    Z_reshaped.append(np.reshape(Z_crt, [608 // 16, 608 // 16]))\n",
    "\n",
    "    \n",
    "# Run post process \n",
    "for ind, label_img in enumerate(Z_reshaped):\n",
    "    label_img = postprocess(label_img)\n",
    "    Z_reshaped[ind] = np.reshape(label_img, [Z_crt.shape[0]])\n",
    "    \n",
    "# Convert list as array\n",
    "result = np.concatenate( Z_reshaped , axis = 0 )\n",
    "\n",
    "# Save prediction\n",
    "create_submission(result, \"submission_postprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "patch_size = 16\n",
    "\n",
    "# Run prediction on the img_idx-th image\n",
    "img_idx = random.randint(0,n-1)\n",
    "img_idx = 1\n",
    "\n",
    "patch_size = 16\n",
    "Xi = extract_img_features(image_dir + files[img_idx])\n",
    "Xi = features_augmentation(Xi)\n",
    "Zi = logreg.predict(Xi)\n",
    "\n",
    "# Display prediction as an image\n",
    "w = gt_imgs[img_idx].shape[0]\n",
    "h = gt_imgs[img_idx].shape[1]\n",
    "\n",
    "predicted_im = label_to_img(w, h, patch_size, patch_size, Zi)\n",
    "cimg = concatenate_images(imgs[img_idx], predicted_im)\n",
    "fig1 = plt.figure(figsize=(10, 10)) # create a figure with the default size \n",
    "plt.imshow(cimg, cmap='Greys_r')\n",
    "\n",
    "new_img = make_img_overlay(imgs[img_idx], predicted_im)\n",
    "\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zi_reshaped = np.reshape(Zi, [400 // 16, 400 // 16])\n",
    "postprocess_img = postprocess(Zi_reshaped)\n",
    "postprocess_img = np.reshape(postprocess_img, [Zi.shape[0]])\n",
    "postprocess_img = label_to_img(w, h, patch_size, patch_size, postprocess_img)\n",
    "    \n",
    "fig1 = plt.figure(figsize=(10, 10)) \n",
    "new_img = make_img_overlay(imgs[img_idx], postprocess_img)\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zi_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 16 # each patch is 16*16 pixels\n",
    "\n",
    "img_patches = [img_crop(imgs[i], patch_size, patch_size, step = 4) for i in range(n)]\n",
    "gt_patches = [img_crop(gt_imgs[i], patch_size, patch_size, step = 4) for i in range(n)]\n",
    "\n",
    "# Linearize list of patches\n",
    "img_patches = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     Y = np.asarray([value_to_class(np.mean(gt_patches[i])) for i in range(len(gt_patches))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X = np.asarray([ extract_features(img_patches[i]) for i in range(len(img_patches))])\n",
    "\n",
    "print(\"shape X[i] : {}\".format(X[0].shape))\n",
    "\n",
    "# Features augmentation \n",
    "X = features_augmentation(X)\n",
    "\n",
    "# Standardize data\n",
    "X -= np.mean(X)\n",
    "X /= np.std(X, axis = 0)\n",
    "\n",
    "# Print feature statistics\n",
    "print('Computed ' + str(X.shape[0]) + ' features')\n",
    "print('Feature dimension = ' + str(X.shape[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
