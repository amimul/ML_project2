{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import os,sys\n",
    "# from PIL import Image\n",
    "\n",
    "from helpers import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import cv2\n",
    "except: \n",
    "    import pip\n",
    "    pip.main(['install', 'opencv-python'])\n",
    "    import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 satellite + ground truth images\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"training/\"\n",
    "image_dir = root_dir + \"images/\"\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "\n",
    "files = os.listdir(image_dir)\n",
    "\n",
    "n = len(files)\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "\n",
    "print(\"Loading \" + str(n) + \" satellite + ground truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = get_rotated_images(imgs,True)\n",
    "# gt_imgs = get_rotated_images(gt_imgs,False)\n",
    "\n",
    "# print(\"After rotation: \" + str(len(imgs)) + \" satellite + ground truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 16 # each patch is 16*16 pixels\n",
    "\n",
    "img_patches = [img_crop_train(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [img_crop_train(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "# img_patches = [img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "# gt_patches = [img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "\n",
    "# Linearize list of patches\n",
    "# img_patches = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "# gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X[i] : (10,)\n",
      "Computed 62500 features\n",
      "Feature dimension = 11\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "X = np.asarray([ extract_features(img_patches[i]) for i in range(len(img_patches))])\n",
    "\n",
    "print(\"shape X[i] : {}\".format(X[0].shape))\n",
    "\n",
    "# Features augmentation \n",
    "X = features_augmentation(X)\n",
    "\n",
    "# X = normalize(X)\n",
    "\n",
    "# Print feature statistics\n",
    "print('Computed ' + str(X.shape[0]) + ' features')\n",
    "print('Feature dimension = ' + str(X.shape[1]))\n",
    "# print('Number of classes = ' + str(np.max(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    \"\"\" \n",
    "    input:  predictions - prediction array\n",
    "            labels      - real labels array\n",
    "    output: acc         - Accuracy: percentage of elements of predictions and labels that are the same\n",
    "            PPV         - Positive Predictive Value, Precision\n",
    "            TPR         - True Positive Rate, Sensitivity\n",
    "    \"\"\"\n",
    "    \n",
    "    conf_mat = confusion_matrix(labels, predictions)\n",
    "\n",
    "    TN = conf_mat[0,0]\n",
    "    FP = conf_mat[0,1]\n",
    "    FN = conf_mat[1,0]\n",
    "    TP = conf_mat[1,1]\n",
    "    \n",
    "    TPR = TP / (TP + FN)\n",
    "    PPV = TP / (TP + FP)\n",
    "    \n",
    "    meanFscore = 2 * PPV * TPR / (PPV + TPR)\n",
    "    F1 = 0\n",
    "    \n",
    "    acc = (TP + TN) / np.sum(conf_mat)\n",
    "    \n",
    "#     print(\"\\nTPR = Sensitivity = {}\".format(TPR))\n",
    "#     print(\"PPV = Precision = {}\".format(PPV))\n",
    "#     print(\"Mean F Score = {}\".format(meanFscore))\n",
    "#     print(\"F1 overall = {}\".format(F1))\n",
    "    \n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\prisgdd\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from plots import cross_validation_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "kf.get_n_splits(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 10000.0\n",
      "Foreground threshold = 0.92\n",
      "\n",
      "1-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "2-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "3-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "4-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "5-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "6-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "7-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "8-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "9-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "\n",
      "10-th CV\n",
      "Logistic Regression // Logistic Regression + Postprocessing\n",
      "Average test accuracy: 0.6455200000000001\n",
      "Variance test accuracy: 0.04209868786553804\n",
      "Min test accuracy: 0.58112\n",
      "Max test accuracy: 0.7096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute features for each image patch\n",
    "# percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "foreground_threshold = np.arange(0.92, 0.921, 0.01) \n",
    "Cs = np.arange(1e4, 1e4 + 1, 10000) \n",
    "acc_threshold =[]\n",
    "\n",
    "accuracy_train_C = []\n",
    "f1_score_train_C = []\n",
    "\n",
    "accuracy_test_C = []\n",
    "f1_score_test_C = []\n",
    "\n",
    "for C in Cs:\n",
    "    print(\"C = {}\".format(C))\n",
    "    for threshold in foreground_threshold:\n",
    "        print(\"Foreground threshold = {}\".format(threshold))\n",
    "        def value_to_class(v):\n",
    "            df = np.sum(v)\n",
    "            if df > threshold:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        Y = np.asarray([value_to_class(np.mean(gt_patches[i])) for i in range(len(gt_patches))])\n",
    "\n",
    "\n",
    "        accuracy_train_CV = []\n",
    "        accuracy_test_CV = []\n",
    "        \n",
    "        f1_score_train_CV = []\n",
    "        f1_score_test_CV = []\n",
    "        \n",
    "        for ind, [train_index, test_index] in enumerate(kf.split(imgs)):\n",
    "\n",
    "            ######## Split dataset ########\n",
    "            print(\"\\n{}-th CV\".format(ind+1))\n",
    "            \n",
    "            X_train = [imgs[ind] for ind in train_index]\n",
    "            X_test = [imgs[ind] for ind in test_index]\n",
    "            \n",
    "            y_train = [gt_imgs[ind] for ind in train_index]\n",
    "            y_test = [gt_imgs[ind] for ind in test_index]\n",
    "            \n",
    "#             # print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "#             X_train, X_test = X[train_index], X[test_index]\n",
    "#             y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "            X_train = [img_crop_train(X_train[i], patch_size, patch_size) for i in range(len(train_index))]\n",
    "            y_train = [img_crop_train(y_train[i], patch_size, patch_size) for i in range(len(train_index))]\n",
    "            X_test = [img_crop(X_test[i], patch_size, patch_size) for i in range(len(test_index))]\n",
    "            y_test = [img_crop(y_test[i], patch_size, patch_size) for i in range(len(test_index))]\n",
    "    \n",
    "            X_train = np.asarray([X_train[i][j] for i in range(len(X_train)) for j in range(len(X_train[i]))])\n",
    "            X_test = np.asarray([X_test[i][j] for i in range(len(X_test)) for j in range(len(X_test[i]))])\n",
    "            y_train = np.asarray([y_train[i][j] for i in range(len(y_train)) for j in range(len(y_train[i]))])\n",
    "            y_test = np.asarray([y_test[i][j] for i in range(len(y_test)) for j in range(len(y_test[i]))])\n",
    "        \n",
    "            y_train = np.asarray([value_to_class(np.mean(y_train[i])) for i in range(y_train.shape[0])])\n",
    "            y_test = np.asarray([value_to_class(np.mean(y_test[i])) for i in range(y_test.shape[0])])\n",
    "            \n",
    "            X_train = np.asarray([ extract_features(X_train[i]) for i in range(len(X_train))])\n",
    "            X_train = features_augmentation(X_train)\n",
    "            \n",
    "            X_test = np.asarray([ extract_features(X_test[i]) for i in range(len(X_test))])\n",
    "            X_test = features_augmentation(X_test)\n",
    "            \n",
    "            \n",
    "            \n",
    "            ######## Run logistic regression ########\n",
    "            print(\"Logistic Regression // Logistic Regression + Postprocessing\")\n",
    "            logreg = linear_model.LogisticRegression(C=C, class_weight=\"balanced\")\n",
    "            logreg.fit(X_train, y_train)\n",
    "            z_train = logreg.predict(X_train)\n",
    "            z_test = logreg.predict(X_test)\n",
    "\n",
    "            f1_score_train = f1_score(y_train, z_train, average='macro')\n",
    "            accuracy_score_train = accuracy_score(y_train, z_train)\n",
    "            \n",
    "            f1_score_test = f1_score(y_test, z_test, average='macro')\n",
    "            accuracy_score_test = accuracy_score(y_test, z_test)\n",
    "\n",
    "            ######## Postprocessing ########\n",
    "            # Reshape prediction\n",
    "            z_reshaped = []\n",
    "\n",
    "            num_patch_total = len(z_test)\n",
    "            num_patch_by_img = num_patch_total // kf.get_n_splits(imgs)\n",
    "\n",
    "            for i in range(0, num_patch_total, num_patch_by_img):\n",
    "                z_crt = z_test[i : i + num_patch_by_img]\n",
    "                z_reshaped.append(np.reshape(z_crt, [400 // 16, 400 // 16]))\n",
    "\n",
    "\n",
    "            # Run post process \n",
    "            for ind, label_img in enumerate(z_reshaped):\n",
    "                label_img = postprocess(label_img)\n",
    "                z_reshaped[ind] = np.reshape(label_img, [z_crt.shape[0]])\n",
    "\n",
    "            # Convert list as array\n",
    "            z_test_pp = np.concatenate( z_reshaped , axis = 0 )\n",
    "\n",
    "            f1_score_test_pp = f1_score(y_test, z_test_pp, average='macro')\n",
    "            accuracy_score_test_pp = accuracy_score(y_test, z_test_pp)\n",
    "#             print(\" - - - F1 score test :: LogReg = {} ::: LogReg + PP = {}\".format(f1_score_test, f1_score_test_pp))\n",
    "#             print(\" - - - Accuracy score test :: LogReg {} ::: LogReg + PP {}\".format(accuracy_score_test, accuracy_score_test_pp))\n",
    "\n",
    "            \n",
    "            f1_score_train_CV.append(f1_score_train)\n",
    "            accuracy_train_CV.append(accuracy_score_train)\n",
    "            \n",
    "            f1_score_test_CV.append(f1_score_test_pp)\n",
    "            accuracy_test_CV.append(accuracy_score_test_pp)\n",
    "    \n",
    "    print(\"Average test accuracy: {}\".format(np.mean(accuracy_test_CV)))\n",
    "    print(\"Variance test accuracy: {}\".format(np.std(accuracy_test_CV)))\n",
    "    print(\"Min test accuracy: {}\".format(np.min(accuracy_test_CV)))\n",
    "    print(\"Max test accuracy: {}\\n\".format(np.max(accuracy_test_CV)))   \n",
    "    \n",
    "    accuracy_train_C.append(np.mean(accuracy_train_CV))\n",
    "    f1_score_train_C.append(np.mean(f1_score_train_CV))\n",
    "    \n",
    "    accuracy_test_C.append(np.mean(accuracy_test_CV))\n",
    "    f1_score_test_C.append(np.mean(f1_score_test_CV))\n",
    "        \n",
    "        \n",
    "# cross_validation_visualization(Cs, f1_score_train_C, f1_score_test_C)\n",
    "#     # we create an instance of the classifier and fit the data\n",
    "#     logreg = linear_model.LogisticRegression(C=1e5, class_weight=\"balanced\")\n",
    "#     logreg.fit(X, Y)\n",
    "    \n",
    "#     # Predict on the training set\n",
    "#     Z = logreg.predict(X)\n",
    "    \n",
    "#     acc = accuracy(labels = Y, predictions = Z)\n",
    "#     acc_threshold.append(acc)\n",
    "#     print(\"Foreground threshold = {}\".format(threshold))\n",
    "#     print(\"Accuracy post Logistic Regression: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data to evaluate\n",
    "root_testdir = \"test_set_images\"\n",
    "test_names = os.listdir(root_testdir)\n",
    "\n",
    "num_test = len(test_names)\n",
    "\n",
    "imgs_test = [load_image(os.path.join(root_testdir, test_names[i], test_names[i]) + \".png\") for i in range(num_test)]\n",
    "\n",
    "img_patches_test = [img_crop(imgs_test[i], patch_size, patch_size) for i in range(num_test)]\n",
    "\n",
    "# Linearize list of patches\n",
    "img_patches_test = np.asarray([img_patches_test[i][j] for i in range(len(img_patches_test)) for j in range(len(img_patches_test[i]))])\n",
    "\n",
    "\n",
    "X_test = np.asarray([ extract_features(img_patches_test[i]) for i in range(len(img_patches_test))])\n",
    "X_test = features_augmentation(X_test)\n",
    "\n",
    "# X_test = normalize(X_test)\n",
    "\n",
    "# Run prediction\n",
    "Z_test = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Postprocessing \"\"\"\n",
    "\n",
    "# Reshape prediction\n",
    "Z_reshaped = []\n",
    "\n",
    "num_patch_total = len(Z_test)\n",
    "num_patch_by_img = num_patch_total // num_test\n",
    "\n",
    "for i in range(0, num_patch_total, num_patch_by_img):\n",
    "    Z_crt = Z_test[i : i + num_patch_by_img]\n",
    "    Z_reshaped.append(np.reshape(Z_crt, [608 // 16, 608 // 16]))\n",
    "\n",
    "    \n",
    "# Run post process \n",
    "for ind, label_img in enumerate(Z_reshaped):\n",
    "    label_img = postprocess(label_img)\n",
    "    Z_reshaped[ind] = np.reshape(label_img, [Z_crt.shape[0]])\n",
    "    \n",
    "# Convert list as array\n",
    "result = np.concatenate( Z_reshaped , axis = 0 )\n",
    "\n",
    "# Save prediction\n",
    "create_submission(result, \"submission_postprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "patch_size = 16\n",
    "\n",
    "# Run prediction on the img_idx-th image\n",
    "img_idx = random.randint(0,n-1)\n",
    "img_idx = 1\n",
    "\n",
    "patch_size = 16\n",
    "Xi = extract_img_features(image_dir + files[img_idx])\n",
    "Xi = features_augmentation(Xi)\n",
    "Zi = logreg.predict(Xi)\n",
    "\n",
    "# Display prediction as an image\n",
    "w = gt_imgs[img_idx].shape[0]\n",
    "h = gt_imgs[img_idx].shape[1]\n",
    "\n",
    "predicted_im = label_to_img(w, h, patch_size, patch_size, Zi)\n",
    "cimg = concatenate_images(imgs[img_idx], predicted_im)\n",
    "fig1 = plt.figure(figsize=(10, 10)) # create a figure with the default size \n",
    "plt.imshow(cimg, cmap='Greys_r')\n",
    "\n",
    "new_img = make_img_overlay(imgs[img_idx], predicted_im)\n",
    "\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zi_reshaped = np.reshape(Zi, [400 // 16, 400 // 16])\n",
    "postprocess_img = postprocess(Zi_reshaped)\n",
    "postprocess_img = np.reshape(postprocess_img, [Zi.shape[0]])\n",
    "postprocess_img = label_to_img(w, h, patch_size, patch_size, postprocess_img)\n",
    "    \n",
    "fig1 = plt.figure(figsize=(10, 10)) \n",
    "new_img = make_img_overlay(imgs[img_idx], postprocess_img)\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zi_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
